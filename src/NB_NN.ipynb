{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy \n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sys import getsizeof\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rfpimp import *\n",
    "\n",
    "scorer = make_scorer(f1_score, needs_proba=False,\n",
    "                     greater_is_better=True,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 148259, 3: 87218, 1: 25124})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/train_values.csv')\n",
    "train_target = pd.read_csv('../input/train_labels.csv')\n",
    "\n",
    "print(Counter(train_target.damage_grade))\n",
    "\n",
    "y = train_target.damage_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def create_X(df):\n",
    "    X = df.copy()\n",
    "    \n",
    "#     X['disc'] = X['ground_floor_type'].astype(str) + X['other_floor_type'].astype(str)  # new feature\n",
    "    \n",
    "    cat_cols = X.columns[X.dtypes == 'object']\n",
    "    id_cols = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "    cat_cols = id_cols + cat_cols.tolist()\n",
    "    num_cols = [i for i in X.columns if i not in cat_cols]\n",
    "    \n",
    "    x1 = X[cat_cols].apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    X = pd.concat([X[num_cols], x1], axis=1)\n",
    "    \n",
    "    return X, num_cols, cat_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, cat_cols=None, output_col=None):\n",
    "        self.n = data.shape[0]\n",
    "\n",
    "        if output_col:\n",
    "            self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
    "        else:\n",
    "            self.y =  np.zeros((self.n, 1))\n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else []\n",
    "        self.cont_cols = [col for col in data.columns\n",
    "                          if col not in self.cat_cols + [output_col]]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].astype(np.float32).values\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].astype(np.int64).values\n",
    "        else:\n",
    "            self.cat_X =  np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "  def __init__(self, emb_dims, no_of_cont, lin_layer_sizes,\n",
    "               output_size, emb_dropout, lin_layer_dropouts):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    emb_dims: List of two element tuples\n",
    "      This list will contain a two element tuple for each\n",
    "      categorical feature. The first element of a tuple will\n",
    "      denote the number of unique values of the categorical\n",
    "      feature. The second element will denote the embedding\n",
    "      dimension to be used for that feature.\n",
    "\n",
    "    no_of_cont: Integer\n",
    "      The number of continuous features in the data.\n",
    "\n",
    "    lin_layer_sizes: List of integers.\n",
    "      The size of each linear layer. The length will be equal\n",
    "      to the total number\n",
    "      of linear layers in the network.\n",
    "\n",
    "    output_size: Integer\n",
    "      The size of the final output.\n",
    "\n",
    "    emb_dropout: Float\n",
    "      The dropout to be used after the embedding layers.\n",
    "\n",
    "    lin_layer_dropouts: List of floats\n",
    "      The dropouts to be used after each linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    # Embedding layers\n",
    "    self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
    "                                     for x, y in emb_dims])\n",
    "\n",
    "    no_of_embs = sum([y for x, y in emb_dims])\n",
    "    self.no_of_embs = no_of_embs\n",
    "    self.no_of_cont = no_of_cont\n",
    "\n",
    "    # Linear Layers\n",
    "    first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont,\n",
    "                                lin_layer_sizes[0])\n",
    "\n",
    "    self.lin_layers =\\\n",
    "     nn.ModuleList([first_lin_layer] +\\\n",
    "          [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "           for i in range(len(lin_layer_sizes) - 1)])\n",
    "    \n",
    "    for lin_layer in self.lin_layers:\n",
    "      nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "    # Output Layer\n",
    "    self.output_layer = nn.Linear(lin_layer_sizes[-1],\n",
    "                                  output_size)\n",
    "    nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "    # Batch Norm Layers\n",
    "    self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "    self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n",
    "                                    for size in lin_layer_sizes])\n",
    "\n",
    "    # Dropout Layers\n",
    "    self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "    self.droput_layers = nn.ModuleList([nn.Dropout(size)\n",
    "                                  for size in lin_layer_dropouts])\n",
    "\n",
    "  def forward(self, cont_data, cat_data):\n",
    "\n",
    "    if self.no_of_embs != 0:\n",
    "      x = [emb_layer(cat_data[:, i])\n",
    "           for i,emb_layer in enumerate(self.emb_layers)]\n",
    "      x = torch.cat(x, 1)\n",
    "      x = self.emb_dropout_layer(x)\n",
    "\n",
    "    if self.no_of_cont != 0:\n",
    "      normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "\n",
    "      if self.no_of_embs != 0:\n",
    "        x = torch.cat([x, normalized_cont_data], 1) \n",
    "      else:\n",
    "        x = normalized_cont_data\n",
    "\n",
    "    for lin_layer, dropout_layer, bn_layer in\\\n",
    "        zip(self.lin_layers, self.droput_layers, self.bn_layers):\n",
    "      \n",
    "      x = F.relu(lin_layer(x))\n",
    "      x = bn_layer(x)\n",
    "      x = dropout_layer(x)\n",
    "\n",
    "    x = self.output_layer(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260601, 40) (260601, 2)\n",
      "[31, 1414, 11595, 3, 5, 3, 5, 4, 4, 10, 4]\n",
      "[(31, 16), (1414, 128), (11595, 128), (3, 2), (5, 3), (3, 2), (5, 3), (4, 2), (4, 2), (10, 5), (4, 2)]\n"
     ]
    }
   ],
   "source": [
    "X, num_cols, cat_cols  = create_X(df)\n",
    "\n",
    "\n",
    "X = pd.merge(X,train_target, on='building_id')\n",
    "print(X.shape, train_target.shape)\n",
    "\n",
    "X['damage_grade'] = X['damage_grade'] - 1\n",
    "\n",
    "# X.drop(columns=['building_id'], axis=1, inplace=True)\n",
    "\n",
    "cat_dims = [int(X[col].nunique()) for col in cat_cols]\n",
    "print(cat_dims)\n",
    "# [15, 5, 2, 4, 112]\n",
    "emb_dims = [(x, min(128, (x + 1) // 2)) for x in cat_dims]\n",
    "print(emb_dims)\n",
    "# [(15, 8), (5, 3), (2, 1), (4, 2), (112, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X[num_cols] = StandardScaler().fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for i in num_cols:\n",
    "    X[i+'_sq'] = X[i]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## no of count\n",
    "X.shape[1]-len(cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=57, lin_layer_sizes=[100, 50],\n",
    "                          output_size=3, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "## tying up everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def score_tensors(y, preds):\n",
    "    preds = preds.argmax(1)\n",
    "    preds = preds.reshape(-1)\n",
    "    y = y.reshape(-1)\n",
    "\n",
    "    preds = preds.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    \n",
    "    score = f1_score(y, preds, average='micro')\n",
    "    \n",
    "    return score\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def f1_micro(true:torch.Tensor, predicted:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. 0 <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    \n",
    "    '''\n",
    "    assert true.ndim == 1\n",
    "    assert predicted.ndim == 1 or predicted.ndim == 2\n",
    "    \n",
    "    if predicted.ndim == 2:\n",
    "        predicted = predicted.argmax(dim=1)\n",
    "    \n",
    "    tp_all = []\n",
    "    tn_all = []\n",
    "    fp_all = []\n",
    "    fn_all = []\n",
    "    \n",
    "    for c in true.unique():\n",
    "        y_true = true.clone()\n",
    "        y_pred = predicted.clone()\n",
    "        \n",
    "        b1 = y_true==c\n",
    "        b0 = y_true!=c\n",
    "        y_true[b1] = 1\n",
    "        y_true[b0] = 0\n",
    "        \n",
    "        b1 = y_pred==c\n",
    "        b0 = y_pred!=c\n",
    "        y_pred[b1] = 1\n",
    "        y_pred[b0] = 0\n",
    "        \n",
    "        \n",
    "        tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "        \n",
    "        tp_all.append(tp); tn_all.append(tn)\n",
    "        fp_all.append(fp); fn_all.append(fn)\n",
    "    \n",
    "    tp_all = torch.stack(tp_all).sum() ; tn_all = torch.stack(tn_all).sum()\n",
    "    fp_all = torch.stack(fp_all).sum() ; fn_all = torch.stack(fn_all).sum()\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    precision = tp_all / (tp_all + fp_all + epsilon)\n",
    "    recall = tp_all / (tp_all + fn_all + epsilon)\n",
    "\n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return -f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. 0 <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    \n",
    "    '''\n",
    "    assert y_true.ndim == 1\n",
    "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
    "    \n",
    "    if y_pred.ndim == 2:\n",
    "        y_pred = y_pred.argmax(dim=1)\n",
    "        \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_indices, val_indices = train_test_split(X.index.values, train_size=0.8, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(data=X.iloc[train_indices], cat_cols=cat_cols,\n",
    "                             output_col='damage_grade')\n",
    "val_dataset = TabularDataset(data=X.iloc[val_indices], cat_cols=cat_cols,\n",
    "                             output_col='damage_grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batchsize, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3258, 815)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ids, _ = train_test_split(X.index.values, train_size=0.001, random_state=99)\n",
    "baby_dataset = TabularDataset(data=X.iloc[ids], cat_cols=cat_cols,\n",
    "                             output_col='damage_grade')\n",
    "baby_dataloader = DataLoader(baby_dataset, batchsize, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for y, cont_x, cat_x in dataloader:\n",
    "#             print(cat_x.device)\n",
    "            cat_x = cat_x.to(device)\n",
    "            cont_x = cont_x.to(device)\n",
    "            y  = y.to(device)\n",
    "            batch_preds = model(cont_x, cat_x)\n",
    "\n",
    "            predictions.append(batch_preds)\n",
    "            actuals.append(y)\n",
    "        \n",
    "    predictions = torch.cat(predictions, 0)\n",
    "    actuals = torch.cat(actuals, 0)\n",
    "    \n",
    "    return score_tensors(actuals, predictions)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for y, cont_x, cat_x in train_dataloader:\n",
    "    cat_x = cat_x.to(device)\n",
    "    cont_x = cont_x.to(device)\n",
    "    y  = y.to(device)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "preds = model(cont_x, cat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=56, lin_layer_sizes=[100, 50],\n",
    "                          output_size=3, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 f1-score = 0.5807692307692308\n",
      "epoch = 1 f1-score = 0.7961538461538461\n",
      "epoch = 2 f1-score = 0.9423076923076923\n",
      "epoch = 3 f1-score = 0.9615384615384616\n",
      "epoch = 4 f1-score = 0.9423076923076923\n",
      "epoch = 5 f1-score = 0.9884615384615385\n",
      "epoch = 6 f1-score = 0.9884615384615385\n",
      "epoch = 7 f1-score = 0.9846153846153847\n",
      "epoch = 8 f1-score = 0.9923076923076923\n",
      "epoch = 9 f1-score = 0.9846153846153847\n",
      "epoch = 10 f1-score = 0.9653846153846154\n",
      "epoch = 11 f1-score = 0.9576923076923077\n",
      "epoch = 12 f1-score = 0.9653846153846154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d4eb475f62b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaby_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch = {epoch} f1-score = {val_score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-14a4e1143c58>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcont_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcont_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mbatch_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0c75bb1b5df4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cont_data, cat_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_of_embs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       x = [emb_layer(cat_data[:, i])\n\u001b[0;32m---> 76\u001b[0;31m            for i,emb_layer in enumerate(self.emb_layers)]\n\u001b[0m\u001b[1;32m     77\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_dropout_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0c75bb1b5df4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_of_embs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       x = [emb_layer(cat_data[:, i])\n\u001b[0;32m---> 76\u001b[0;31m            for i,emb_layer in enumerate(self.emb_layers)]\n\u001b[0m\u001b[1;32m     77\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_dropout_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_of_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "for epoch in range(no_of_epochs):\n",
    "    for y, cont_x, cat_x in baby_dataloader:\n",
    "\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "#         print(y.ndim,y.shape\n",
    "        loss = criterion(target=y.long().reshape(-1), input=preds)\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_score = validate(model, baby_dataloader) \n",
    "    print(f\"epoch = {epoch} f1-score = {val_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a91be68e0c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "batchsize = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batchsize, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31, 31), (1414, 64), (11595, 64), (3, 3), (5, 5), (3, 3), (5, 5), (4, 4), (4, 4), (10, 10), (4, 4)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.568912\n",
       "2    0.334680\n",
       "0    0.096408\n",
       "Name: damage_grade, dtype: float64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.damage_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weights_tensor = torch.Tensor([0.1,0.55,0.34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weights_tensor = weights_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weights_tensor = 1/weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-12da3c3bb3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'weights_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "weights_tensor = weights_tensor.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-7f1a542b88f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mno_of_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "no_of_epochs = 30\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor,)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31, 62), (1414, 512), (11595, 512), (3, 6), (5, 10), (3, 6), (5, 10), (4, 8), (4, 8), (10, 20), (4, 8)]\n"
     ]
    }
   ],
   "source": [
    "emb_dims = [(x, min(512, 2*x)) for x in cat_dims]\n",
    "print(emb_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=56, lin_layer_sizes=[100, 50],\n",
    "                          output_size=3, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90af103b4587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcat_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    train_loss = 0\n",
    "    for y, cont_x, cat_x in train_dataloader:\n",
    "\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(target=y.long().reshape(-1), input=preds)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_score = validate(model, val_dataloader) \n",
    "    print(f\"epoch = {epoch} f1-score = {val_score}\")\n",
    "    print(f\"train loss = {train_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "no_of_epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "for epoch in range(no_of_epochs):\n",
    "    for y, cont_x, cat_x in train_dataloader:\n",
    "\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y.long().reshape(-1))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_score = validate(model, val_dataloader) \n",
    "    print(f\"epoch = {epoch} f1-score = {val_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batchsize = 2048\n",
    "train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batchsize, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=27, lin_layer_sizes=[100, 50],\n",
    "                          output_size=3, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "no_of_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(no_of_epochs):\n",
    "    for y, cont_x, cat_x in train_dataloader:\n",
    "\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y.long().reshape(-1))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_score = validate(model, val_dataloader) \n",
    "    print(f\"epoch = {epoch} f1-score = {val_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=27, lin_layer_sizes=[100, 50],\n",
    "                          output_size=3, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "no_of_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "for epoch in range(no_of_epochs):\n",
    "    for y, cont_x, cat_x in train_dataloader:\n",
    "\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y.long().reshape(-1))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_score = validate(model, val_dataloader) \n",
    "    print(f\"epoch = {epoch} f1-score = {val_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=27, lin_layer_sizes=[100, 50],\n",
    "                          output_size=3, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.1,0.1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batchsize = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=1)\n",
    "val_dataloader = DataLoader(val_dataset, batchsize, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd24b82d3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd24b82d3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd24b82d3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/data/nithish/anaconda3/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-5e8af5f9ae10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Backward Pass and Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_of_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(no_of_epochs):\n",
    "    for y, cont_x, cat_x in train_dataloader:\n",
    "\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y.long().reshape(-1))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_score = validate(model, val_dataloader) \n",
    "    print(f\"epoch = {epoch} f1-score = {val_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
